{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RutujKhare1/CS6910_Assignment_1/blob/main/FDL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGFo5cj3nU9X"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OGNmUu8HCDUH"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "wandb.login(key = '5b3ff6cba361172038b8948f6dace9286a5bbfa0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9PzAPWydDWfc"
      },
      "outputs": [],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "num_label = 10\n",
        "num_samples = len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuyyHY0_EInl",
        "outputId": "ffccafe6-e330-43de-8aa7-4956257712cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape : (60000, 28, 28), (60000,)\n",
            "Test shape : (10000, 28, 28), (10000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Train shape : {}, {}\".format(X_train.shape, Y_train.shape))\n",
        "print(\"Test shape : {}, {}\".format(X_test.shape, Y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPTwJqSlPRfI"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UkdOq_aE8MP"
      },
      "outputs": [],
      "source": [
        "num_row = 2\n",
        "num_col = 5\n",
        "\n",
        "cnt = 0\n",
        "images = X_train\n",
        "labels = Y_train\n",
        "dic = {0:'T-shirt', 1:'Trowser', 2:'Pullover', 3:'Dress', 4:'Coat', 5:'Sandal', 6:'Shirt', 7:'Sneaker', 8:'Bag', 9:'Ankle Boot'}\n",
        "temp_labels = []\n",
        "r_idx = 0\n",
        "c_idx = 0\n",
        "fig, axes = plt.subplots(num_row, num_col, figsize=(2*num_col, 2*num_row))\n",
        "idx = 0\n",
        "while (cnt<10 and idx < 60000):\n",
        "  if(labels[idx] not in temp_labels):\n",
        "    if(c_idx == num_col):\n",
        "      r_idx = 1\n",
        "      c_idx = 0\n",
        "    wandb.init(project = \"dl-project_images\")\n",
        "    wandb.log({'Images' : [wandb.Image(images[idx], caption = dic[labels[idx]])]})\n",
        "    temp_labels.append([labels[idx]])\n",
        "    ax = axes[r_idx, c_idx]\n",
        "    ax.imshow(images[idx], cmap='gray')\n",
        "    ax.set_title('{}'.format(dic[labels[idx]]))\n",
        "    c_idx += 1\n",
        "    cnt += 1\n",
        "  idx += 1 \n",
        "wandb.finish()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqNF9yyP395T"
      },
      "source": [
        "#Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lPkflwJW3zTh"
      },
      "outputs": [],
      "source": [
        "#Helper functions\n",
        "\n",
        "def xavierRandom(x, y):\n",
        "  limit = np.sqrt(6 / float(x + y))\n",
        "  return np.random.uniform(low=-limit, high=limit, size=(x,y))\n",
        "  \n",
        "#activation functions\n",
        "\n",
        "def sigmoid(x):\n",
        "  z = x.copy()\n",
        "  z[x < 0] = np.exp(x[x < 0])/(1 + np.exp(x[x<0]))\n",
        "  z[x >= 0] = 1/(1+np.exp(-x[x >= 0]))\n",
        "  return z\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return sigmoid(x)*(1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "  e_x = np.exp(x - max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "  \n",
        "def d_tanh(x):\n",
        "  return 1 - tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def d_relu(x):\n",
        "  z = x.copy()\n",
        "  z[x < 0]=0\n",
        "  z[x > 0]=1\n",
        "  return z\n",
        "\n",
        "actFn_dict = {\"sigmoid\":sigmoid,\n",
        "               \"softmax\":softmax,\n",
        "               \"relu\":relu,\n",
        "               \"tanh\":tanh}\n",
        "d_actFn_dict = {\"sigmoid\":d_sigmoid,\n",
        "               \"relu\":d_relu,\n",
        "               \"tanh\":d_tanh}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FXf2C9BY4kEN"
      },
      "outputs": [],
      "source": [
        "class OptimizerHelper:\n",
        "  def forwardPropagation(self, W, B, X, num_layer, activation):\n",
        "    H=list()\n",
        "    A=list()\n",
        "    a = np.array(W[0]@X.T + B[0])\n",
        "    h = actFn_dict[activation](a)\n",
        "    H.append(h)\n",
        "    A.append(a)\n",
        "    cur_ip = h\n",
        "\n",
        "    for i in range(num_layer-1):\n",
        "      a = np.array(W[i+1]@cur_ip + B[i+1])\n",
        "      h = actFn_dict[activation](a.T).T\n",
        "      H.append(h)\n",
        "      A.append(a)\n",
        "      cur_ip = h\n",
        "    a = np.array(W[num_layer]@cur_ip + B[num_layer])\n",
        "    h = a.T\n",
        "    A.append(a)\n",
        "    y = np.array([actFn_dict[\"softmax\"](i) for i in h]).T\n",
        "    return H, A, y\n",
        "\n",
        "  def backwardPropagation(self, H, A, y, W, B, X, Y, num_layer, num_label, activation, wt_decay, loss_fn):\n",
        "    xdim = X.shape[0]\n",
        "    d_aL = y\n",
        "    if(loss_fn == \"crossEntropy\"):\n",
        "      ey = np.zeros([xdim,num_label], dtype= int)\n",
        "      for i in range(xdim):\n",
        "        ey[i][Y[i]] = 1\n",
        "      d_aL = -(ey.T - y) \n",
        "    else:\n",
        "      ey = np.zeros([xdim,num_label], dtype= int)\n",
        "      for i in range(xdim):\n",
        "        ey[i][Y[i]] = 1\n",
        "      temp = -(ey.T - y)\n",
        "      d_aL = 2*temp*y*(1-y)\n",
        "\n",
        "    D_W = list()\n",
        "    D_B = list()\n",
        "    for k in reversed(range(1,num_layer+1)):\n",
        "      d_wL = d_aL@H[k-1].T\n",
        "      D_W.append(d_wL/xdim)\n",
        "\n",
        "      d_bL = np.sum(d_aL, axis = 1).reshape(len(d_aL),1)/xdim\n",
        "      D_B.append(d_bL)\n",
        "  \n",
        "      d_hL = W[k].T@d_aL\n",
        "      dga = d_actFn_dict[activation](A[k-1])\n",
        "      d_aL = np.multiply(d_hL,dga)\n",
        "\n",
        "    d_wL = d_aL @ X\n",
        "    d_bL = d_aL\n",
        "    d_bL = np.sum(d_bL, axis = 1).reshape(len(d_aL),1)/xdim\n",
        "    D_W.append(d_wL/xdim)\n",
        "    D_B.append(d_bL)\n",
        "    return D_W, D_B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0cJ2D5tLYICu"
      },
      "outputs": [],
      "source": [
        "X_mini = np.reshape(X_train, (60000,784))/255.0\n",
        "num_data_points = 60000\n",
        "num_labels = 10\n",
        "num_val_points = num_data_points//10\n",
        "num_train_points = num_data_points - num_val_points\n",
        "Xtrain = X_mini[:num_train_points]\n",
        "Ytrain = Y_train[:num_train_points]\n",
        "X_valid = X_mini[num_train_points:]\n",
        "Y_valid =  Y_train[num_train_points:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3"
      ],
      "metadata": {
        "id": "4k0h9RwHVYoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wN7Z0v4JaPPe"
      },
      "outputs": [],
      "source": [
        "class feedforwardNeuralNetwork:\n",
        "  def __init__(self, num_layers = 3, hidden_size = 64, num_labels = 10, neta = 0.1, epochs = 10, X_train = X_train, Y_train = Y_train, batch = 64, w_init = \"xavier\", X_val = X_valid, Y_val = Y_valid, activation = 'sigmoid', wt_decay = 0, loss_fn = \"crossEntropy\"):\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.xdim = X_train.shape[0]\n",
        "    self.ydim = X_train.shape[1]\n",
        "    self.num_labels = num_labels\n",
        "    self.neta = neta\n",
        "    self.epochs = epochs\n",
        "    self.X_train = X_train\n",
        "    self.Y_train = Y_train\n",
        "    self.X_val = X_val\n",
        "    self.Y_val = Y_val\n",
        "    self.batch = batch\n",
        "    w, b = self.initialiseWeight(w_init)\n",
        "    self.W = w\n",
        "    self.B = b\n",
        "    self.activation = activation\n",
        "    self.wt_decay = wt_decay\n",
        "    self.loss_fn = loss_fn\n",
        "  \n",
        "  def initialiseWeight(self, method):\n",
        "    w = list()\n",
        "    b = list()\n",
        "    if(method == \"xavier\"):\n",
        "      w.append(xavierRandom(self.hidden_size, self.ydim))\n",
        "      b.append(np.zeros([self.hidden_size,1]))\n",
        "      for i in range(self.num_layers-1):\n",
        "        w.append(xavierRandom(self.hidden_size, self.hidden_size))\n",
        "        b.append(np.zeros([self.hidden_size,1]))\n",
        "      w.append(xavierRandom(self.num_labels, self.hidden_size))\n",
        "      b.append(np.zeros([self.num_labels,1]))\n",
        "    else:\n",
        "      # w.append(np.random.randn(hidden_size, ydim))\n",
        "      w.append(np.random.uniform(-1,1, (self.hidden_size, self.ydim)))\n",
        "\n",
        "      # b.append(np.random.randn(hidden_size,1))\n",
        "      b.append(np.zeros([self.hidden_size,1]))\n",
        "      for i in range(self.num_layers-1):\n",
        "        # w.append(np.random.randn(hidden_size, hidden_size))\n",
        "        w.append(np.random.uniform(-1,1, (self.hidden_size, self.hidden_size)))\n",
        "        # b.append(np.random.randn(hidden_size,1))\n",
        "        b.append(np.zeros([self.hidden_size,1]))\n",
        "      # w.append(np.random.randn(num_labels, hidden_size))\n",
        "      w.append(np.random.uniform(-1,1, (self.num_labels, self.hidden_size)))\n",
        "      # b.append(np.random.randn(num_labels,1))\n",
        "      b.append(np.zeros([self.num_labels,1])) \n",
        "    return w, b\n",
        "\n",
        "  def calculateCrossEntropy(self, y_hat, Y):\n",
        "    num_data = y_hat.shape[1]\n",
        "    Y_one_hot = np.eye(self.num_labels)[Y]\n",
        "    loss = np.sum(Y_one_hot * np.log(y_hat.T + 1e-15) )\n",
        "    w_sq = 0\n",
        "    for i in range(len(self.W)):\n",
        "      w_sq += np.sum(self.W[i]**2)\n",
        "    return (-loss + (self.wt_decay*w_sq)/2)/(num_data)\n",
        "\n",
        "  def calculateMSE(self, y_hat, Y):\n",
        "    m,n = y_hat.shape\n",
        "    Y_one_hot = np.eye(self.num_labels)[Y]\n",
        "    temp = (y_hat.T - Y_one_hot)**2\n",
        "    loss = np.sum(temp)\n",
        "    w_sq = 0\n",
        "    for i in range(len(self.W)):\n",
        "      w_sq += np.sum(self.W[i]**2)\n",
        "    return (loss + (self.wt_decay*w_sq)/2)/(n)\n",
        "\n",
        "  def calculateAccuracy(self, y_hat, Y):\n",
        "    pred = np.argmax(y_hat, axis=0)\n",
        "    accuracy = np.mean(pred==Y)\n",
        "    return accuracy*100\n",
        "\n",
        "  def feedForward(self, x):\n",
        "    a = np.array(self.W[0]@x.T + self.B[0])\n",
        "    h = actFn_dict[self.activation](a.T).T\n",
        "    cur_ip = h\n",
        "\n",
        "    for i in range(self.num_layers-1):\n",
        "      a = np.array(self.W[i+1]@cur_ip + self.B[i+1])\n",
        "      h = actFn_dict[self.activation](a.T).T\n",
        "      cur_ip = h\n",
        "    a = np.array(self.W[self.num_layers]@cur_ip + self.B[self.num_layers])\n",
        "    h = a.T\n",
        "    # print(h)\n",
        "    y = np.array([actFn_dict[\"softmax\"](i) for i in h]).T\n",
        "    # print(y)\n",
        "    return y\n",
        "\n",
        "  \n",
        "\n",
        "  def momentumGD(self, beta = 0.9, eta = 1.0):\n",
        "    # print(\"neta \",self.neta)\n",
        "    O = OptimizerHelper()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        if k==0:\n",
        "          uw = D_W\n",
        "          ub = D_B\n",
        "        else:\n",
        "          uw = beta*prev_uw + eta*D_W\n",
        "          ub = beta*prev_ub + eta*D_B\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta*uw[i] - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta*ub[i] - self.neta*self.wt_decay*self.B[i]\n",
        "        prev_uw = uw\n",
        "        prev_ub = ub \n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "      print(\"Epoch {} : Loss:{}\\t Accuracy:{}\".format(k, train_loss[-1], train_acc[-1]))\n",
        "      \n",
        "      \n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "  def nagGD(self, beta = 0.1, eta = 1.0):\n",
        "    O = OptimizerHelper()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        if k==0:\n",
        "          H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "          D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        else:\n",
        "          u_w = beta*prev_uw\n",
        "          u_b = beta*prev_ub\n",
        "          temp_u_w = list()\n",
        "          temp_u_b = list()\n",
        "          for i in range(len(self.W)):\n",
        "            temp_u_w.append(self.W[i]-u_w[i])\n",
        "            temp_u_b.append(self.B[i]-u_b[i])  \n",
        "          H, A, y = O.forwardPropagation(temp_u_w, temp_u_b, X_batch, self.num_layers, self.activation)\n",
        "          D_W, D_B = O.backwardPropagation(H, A, y, temp_u_w, temp_u_b, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation)\n",
        "\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        if k==0:\n",
        "          uw = D_W\n",
        "          ub = D_B\n",
        "        else:\n",
        "          uw = beta*prev_uw + eta*D_W\n",
        "          ub = beta*prev_ub + eta*D_B\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta*uw[i] - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta*ub[i] - self.neta*self.wt_decay*self.B[i]\n",
        "        prev_uw = uw\n",
        "        prev_ub = ub \n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "    \n",
        "  \n",
        "  def stochasticGD(self):\n",
        "    O = OptimizerHelper()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        # print(self.B[0].shape)\n",
        "        # print(D_B[0].shape)\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta*D_W[i] - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta*D_B[i] - self.neta*self.wt_decay*self.B[i]\n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "  def RMSprop(self, beta = 0.9, eps = 1e-8):\n",
        "    # print(\"neta \",self.neta)\n",
        "    O = OptimizerHelper()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        if k==0:\n",
        "          uw = (1-beta)*np.square(D_W)\n",
        "          ub = (1-beta)*np.square(D_B)\n",
        "        else:\n",
        "          uw = beta*prev_uw + (1-beta)*np.square(D_W)\n",
        "          ub = beta*prev_ub + (1-beta)*np.square(D_B)\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta*D_W[i]/(np.sqrt(uw[i]) + eps) - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta*D_B[i]/(np.sqrt(ub[i]) + eps) - self.neta*self.wt_decay*self.B[i]\n",
        "        prev_uw = uw\n",
        "        prev_ub = ub \n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "  def adam(self, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
        "    # print(\"neta \",self.neta)\n",
        "    O = OptimizerHelper()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        if k==0:\n",
        "          mw = (1-beta1)*D_W\n",
        "          mb = (1-beta1)*D_B\n",
        "          uw = (1-beta2)*np.square(D_W)\n",
        "          ub = (1-beta2)*np.square(D_B)\n",
        "        else:\n",
        "          mw = beta1*mw + (1-beta1)*D_W\n",
        "          mb = beta1*mb + (1-beta1)*D_B\n",
        "          uw = beta2*uw + (1-beta2)*np.square(D_W)\n",
        "          ub = beta2*ub + (1-beta2)*np.square(D_B)\n",
        "\n",
        "        mw_hat = mw / (1-np.power(beta1,k+1))\n",
        "        mb_hat = mb / (1-np.power(beta1,k+1))\n",
        "        uw_hat = uw / (1-np.power(beta2,k+1))\n",
        "        ub_hat = ub / (1-np.power(beta2,k+1))\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta*mw_hat[i]/(np.sqrt(uw_hat[i]) + eps) - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta*mb_hat[i]/(np.sqrt(ub_hat[i]) + eps) - self.neta*self.wt_decay*self.B[i]\n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "      print(\"Epoch {} : Loss:{}\\t Accuracy:{}\".format(k, train_loss[-1], train_acc[-1]))\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "  def nAdam(self, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
        "    # print(\"neta \",self.neta)\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    O = OptimizerHelper()\n",
        "    for k in range(self.epochs):\n",
        "      for b in range(0, self.xdim, self.batch):\n",
        "        X_batch = self.X_train[b:b+self.batch]\n",
        "        Y_batch = self.Y_train[b:b+self.batch]\n",
        "        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)\n",
        "        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)\n",
        "        D_W = np.flip(D_W)\n",
        "        D_B = np.flip(D_B)\n",
        "        if k==0:\n",
        "          mw = (1-beta1)*D_W\n",
        "          mb = (1-beta1)*D_B\n",
        "          uw = (1-beta2)*np.square(D_W)\n",
        "          ub = (1-beta2)*np.square(D_B)\n",
        "        else:\n",
        "          mw = beta1*mw + (1-beta1)*D_W\n",
        "          mb = beta1*mb + (1-beta1)*D_B\n",
        "          uw = beta2*uw + (1-beta2)*np.square(D_W)\n",
        "          ub = beta2*ub + (1-beta2)*np.square(D_B)\n",
        "\n",
        "        mw_hat = mw / (1-np.power(beta1,k+1))\n",
        "        mb_hat = mb / (1-np.power(beta1,k+1))\n",
        "        uw_hat = uw / (1-np.power(beta2,k+1))\n",
        "        ub_hat = ub / (1-np.power(beta2,k+1))\n",
        "        for i in range(self.num_layers+1):\n",
        "          self.W[i] = self.W[i] - self.neta/(np.sqrt(uw_hat[i]) + eps)*(beta1*mw_hat[i] + (1-beta1)*D_W[i] / (1-np.power(beta1,k+1))) - self.neta*self.wt_decay*self.W[i]\n",
        "          self.B[i] = self.B[i] - self.neta/(np.sqrt(ub_hat[i]) + eps)*(beta1*mb_hat[i] + (1-beta1)*D_B[i] / (1-np.power(beta1,k+1))) - self.neta*self.wt_decay*self.B[i] \n",
        "      \n",
        "      y_train_hat = self.feedForward(self.X_train)\n",
        "      y_val_hat = self.feedForward(self.X_val)\n",
        "      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))\n",
        "      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))\n",
        "      if(self.loss_fn == \"crossEntropy\"):\n",
        "        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))\n",
        "      else:\n",
        "        # print(\"---------MSE-------------\")\n",
        "        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))\n",
        "        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))\n",
        "      # print(\"Epoch {} : Loss:{}\\t Accuracy:{}\".format(k, train_loss[-1], train_acc[-1]))\n",
        "    return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "\n",
        "  def accuracy(self, Y):\n",
        "    pred = np.argmax(Y, axis=1)\n",
        "    print(pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4"
      ],
      "metadata": {
        "id": "VWY6gdBxVsiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "1Pk3-mQrpedl"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  wandb.init()\n",
        "  num_layers = wandb.config.num_hidden_layer\n",
        "  hidden_size = wandb.config.hidden_size\n",
        "  activation = wandb.config.activation\n",
        "  epochs = wandb.config.epochs\n",
        "  optimizer_name = wandb.config.optimizer\n",
        "  neta = wandb.config.lr\n",
        "  batch = wandb.config.batch_size\n",
        "  w_init = wandb.config.weight_init\n",
        "  wt_decay = wandb.config.wt_decay\n",
        "  loss_fn = wandb.config.loss_fn\n",
        "\n",
        "  wandb.run.name = f'hln_{num_layers}_hls_{hidden_size}_op_{optimizer_name}_hla_{activation}_lr_{neta}_ep_{epochs}_bs_{batch}_winit_{w_init}_wt_decay_{wt_decay}'\n",
        "\n",
        "  F = feedforwardNeuralNetwork(neta = neta, hidden_size = hidden_size, num_layers = num_layers, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation = activation, batch = batch, epochs = epochs, w_init = w_init, wt_decay = wt_decay, loss_fn = loss_fn)\n",
        "  if(optimizer_name == \"sgd\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.stochasticGD()\n",
        "  elif (optimizer_name == \"momentum\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.momentumGD()\n",
        "  elif (optimizer_name == \"nesterov\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.nagGD()\n",
        "  elif (optimizer_name == \"rmsprop\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.RMSprop()\n",
        "  elif (optimizer_name == \"adam\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.adam()\n",
        "  elif (optimizer_name == \"nadam\"):\n",
        "    train_loss, train_acc, val_loss, val_acc = F.nAdam()\n",
        "\n",
        "  for i in range(len(train_loss)):\n",
        "    wandb.log({'training_loss': train_loss[i],\n",
        "              'training_accuracy': train_acc[i],\n",
        "              'validation_loss': val_loss[i],\n",
        "              'validation_accuracy': val_acc[i]\n",
        "              })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcAQTKJnp25-"
      },
      "outputs": [],
      "source": [
        "sweep_configuration = {\n",
        "    'method': 'random',\n",
        "    'name': 'dl-project',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'validation_loss'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'epochs': {'values': [10, 15]},\n",
        "        'num_hidden_layer':{'values' : [3, 4, 5]},\n",
        "        'hidden_size': {'values' : [32, 64, 128]},\n",
        "        'lr': {'values' : [1e-2, 1e-3, 1e-4]},\n",
        "        'optimizer':{'values' : ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'weight_init':{'values' : ['random','xavier']},\n",
        "        'activation' : {'values' : ['sigmoid','tanh','relu']},\n",
        "        'wt_decay': {'values' : [0, 0.0001, 0.001]},\n",
        "        'loss_fn': {'values' : ['crossEntropy']}\n",
        "     }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project='dl-project_FINALV2')\n",
        "wandb.agent(sweep_id, function=train)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7\n"
      ],
      "metadata": {
        "id": "TQLck06cV3Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best Hyper-parameter\n",
        "F = feedforwardNeuralNetwork(neta = 0.001, hidden_size = 128, num_layers = 4, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"tanh\", epochs = 15, wt_decay = 0.0, batch = 64)\n",
        "train_loss, train_acc, val_loss, val_acc = F.adam()\n",
        "print(train_loss)\n",
        "print(train_acc)\n",
        "\n",
        "Xtest = np.reshape(X_test, (len(X_test),784))/255\n",
        "y_hat = F.feedForward(Xtest)\n",
        "print(\"Test Accuracy : {}\".format(F.calculateAccuracy(y_hat, Y_test)))"
      ],
      "metadata": {
        "id": "vZA3xMffWLGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotConfusionMatrixUpd(y_true, y_pred):\n",
        "  labels = [i for i in range(num_labels)]\n",
        "  cm = np.zeros((len(labels), len(labels)), dtype=np.int32)\n",
        "  for i in range(len(y_true)):\n",
        "      true_class_index = np.where(labels == y_true[i])[0][0]\n",
        "      pred_class_index = np.where(labels == y_pred[i])[0][0]\n",
        "      cm[true_class_index][pred_class_index] += 1\n",
        "  cm = np.flip(cm,0)\n",
        "  cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(cm_normalized, cmap=plt.cm.Blues, interpolation='nearest')\n",
        "  cbar = ax.figure.colorbar(im, ax=ax)\n",
        "  ax.set_xticks(labels)\n",
        "  ax.set_yticks(labels)\n",
        "  ax.set_xticklabels(labels)\n",
        "  ax.set_yticklabels(labels[::-1])\n",
        "  plt.setp(ax.get_xticklabels(), ha=\"right\",\n",
        "            rotation_mode=\"anchor\")\n",
        "  for i in range(len(labels)):\n",
        "      for j in range(len(labels)):\n",
        "          text = ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                          ha=\"center\", va=\"center\", color=\"silver\")\n",
        "\n",
        "  ax.set_xlabel('Predicted labels')\n",
        "  ax.set_ylabel('True labels')\n",
        "  ax.set_title('Confusion matrix')\n",
        "  # wandb.init(project='dl-assignment-cnfmatFN')\n",
        "  # wandb.log({'Confusion Matrix' : [wandb.Image(im)]})\n",
        "  # wandb.finish()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2ukVvqbSV6yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = np.reshape(X_test, (len(X_test),784))/255\n",
        "y_pred = F.feedForward(Xtest)\n",
        "y_test_pred = np.argmax(y_pred, axis=0)\n",
        "plotConfusionMatrixUpd(y_test_pred, Y_test)"
      ],
      "metadata": {
        "id": "WXunppATV7oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8 "
      ],
      "metadata": {
        "id": "2TVW2KESWWl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_sweep_configuration = {\n",
        "    'method': 'random',\n",
        "    'name': 'dl-project',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'validation_loss'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'batch_size': {'values': [64]},\n",
        "        'epochs': {'values': [15]},\n",
        "        'num_hidden_layer':{'values' : [4]},\n",
        "        'hidden_size': {'values' : [128]},\n",
        "        'lr': {'values' : [1e-3]},\n",
        "        'optimizer':{'values' : ['adam']},\n",
        "        'weight_init':{'values' : ['xavier']},\n",
        "        'activation' : {'values' : ['tanh']},\n",
        "        'wt_decay': {'values' : [0.0001]},\n",
        "        'loss_fn': {'values' : ['crossEntropy', 'MSE']}\n",
        "     }\n",
        "}"
      ],
      "metadata": {
        "id": "91uCbLsQhnnD"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=best_sweep_configuration, project='dl-project_CE_MSE2')\n",
        "wandb.agent(sweep_id, function=train, count = 2)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "GDhHtTqrifmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBmsqiGSrDjD"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.001, hidden_size = 32, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"relu\", batch = 32, epochs = 5, wt_decay = 0.005)\n",
        "# train_loss, train_acc, val_loss, val_acc = F.momentumGD()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.001, hidden_size = 32, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"relu\", batch = 32, epochs = 5, wt_decay = 0.005, loss_fn = \"mse\")\n",
        "# train_loss, train_acc, val_loss, val_acc = F.momentumGD()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ],
      "metadata": {
        "id": "VwTeZCQJSrHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8wcZbK8zLdm"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.05, hidden_size = 64, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"tanh\")\n",
        "# train_loss, train_acc, val_loss, val_acc = F.stochasticGD()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8JYg1SrdvhP"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.05, hidden_size = 64, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"tanh\")\n",
        "# train_loss, train_acc, val_loss, val_acc = F.nagGD()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtJC8xrVCoRV"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.0001, hidden_size = 32, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"relu\")\n",
        "# train_loss, train_acc, val_loss, val_acc = F.RMSprop()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-D6ZoAUheCy"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.05, hidden_size = 32, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"sigmoid\", wt_decay = 0.0)\n",
        "# train_loss, train_acc, val_loss, val_acc = F.adam()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLiyt27pYymX"
      },
      "outputs": [],
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.001, hidden_size = 64, num_layers = 3, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"tanh\", epochs = 10, wt_decay = 0.0001)\n",
        "# train_loss, train_acc, val_loss, val_acc = F.nAdam()\n",
        "# print(train_loss)\n",
        "# print(train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F = feedforwardNeuralNetwork(neta = 0.001, hidden_size = 128, num_layers = 4, X_train = Xtrain, Y_train = Ytrain, X_val = X_valid, Y_val = Y_valid, activation=\"tanh\", epochs = 15, wt_decay = 0.0, batch = 64, loss_fn = \"MSE\")\n",
        "# train_loss, train_acc, val_loss, val_acc = F.adam()\n",
        "# print(train_loss)\n",
        "# print(train_acc)\n",
        "# #best hyper parameter MSE"
      ],
      "metadata": {
        "id": "IYYj7qEgnwRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6wgKJ1bWkwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtLGGXj0SL3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z9QqEIjY1G-"
      },
      "outputs": [],
      "source": [
        "# X_mini = np.reshape(X_train, (60000,784))/255\n",
        "# num_data_points = 32000\n",
        "# X_mini = X_mini[:num_data_points]\n",
        "# Y_mini = Y_train[:num_data_points]\n",
        "# F = feedforwardNeuralNetwork(neta = 0.05, hidden_size = 32, num_layers = 3, X_train = X_mini, Y_train = Y_mini)\n",
        "# F.nagGD()\n",
        "\n",
        "# y_hat = F.feedForward(X_mini)\n",
        "# pred = np.argmax(y_hat, axis=0)\n",
        "# # print(Y_test[:5])\n",
        "# # print(pred)\n",
        "# accuracy = np.mean(pred==Y_mini)\n",
        "# print(\"Train Accuracy : {}\".format(accuracy))\n",
        "\n",
        "# Xtest = np.reshape(X_test, (10000,784))/255\n",
        "# y_hat = F.feedForward(Xtest)\n",
        "# pred = np.argmax(y_hat, axis=0)\n",
        "# # print(Y)\n",
        "# # print(pred)\n",
        "# accuracy = np.mean(pred==Y_test)\n",
        "# print(\"Test Accuracy : {}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/DLA1/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxuXj6Q7JH20",
        "outputId": "511719e3-d8eb-479a-fc3d-a5ae14d97876"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-19 14:43:46.340023: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 14:43:46.340159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-19 14:43:46.340183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:334: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  m = asarray(m)\n",
            "Epoch 0 : Loss:0.42083038393627836\t Accuracy:84.81111111111112\n",
            "Epoch 1 : Loss:0.3238346333750556\t Accuracy:88.1962962962963\n",
            "Epoch 2 : Loss:0.30374899846307524\t Accuracy:88.87962962962963\n",
            "Epoch 3 : Loss:0.2884896452203089\t Accuracy:89.42962962962963\n",
            "Epoch 4 : Loss:0.27603244434635626\t Accuracy:89.82962962962962\n",
            "Epoch 5 : Loss:0.2658658018162194\t Accuracy:90.18518518518519\n",
            "Epoch 6 : Loss:0.25724527506766276\t Accuracy:90.51481481481481\n",
            "Epoch 7 : Loss:0.24963931890766614\t Accuracy:90.82222222222222\n",
            "Epoch 8 : Loss:0.2427222441497969\t Accuracy:91.06666666666666\n",
            "Epoch 9 : Loss:0.23628799872628264\t Accuracy:91.32222222222222\n",
            "Epoch 10 : Loss:0.2302069773434968\t Accuracy:91.5537037037037\n",
            "Epoch 11 : Loss:0.22439501938915402\t Accuracy:91.73703703703704\n",
            "Epoch 12 : Loss:0.2188025802230648\t Accuracy:91.94074074074075\n",
            "Epoch 13 : Loss:0.21340972446702122\t Accuracy:92.11851851851853\n",
            "Epoch 14 : Loss:0.2082107199379977\t Accuracy:92.30185185185185\n",
            "Test Accuracy : 88.39\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_QkY-jznGhtr1lv3UN6yBXOvR7ENmDBF",
      "authorship_tag": "ABX9TyMGekNZT5rnougC3TDWdtWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}