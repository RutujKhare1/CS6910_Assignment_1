# -*- coding: utf-8 -*-
"""FDL_Assignment_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_QkY-jznGhtr1lv3UN6yBXOvR7ENmDBF
"""

from keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import matplotlib.image as img
import numpy as np
import wandb



# print("Train shape : {}, {}".format(X_train.shape, Y_train.shape))
# print("Test shape : {}, {}".format(X_test.shape, Y_test.shape))


#Helper functions

def xavierRandom(x, y):
  limit = np.sqrt(6 / float(x + y))
  return np.random.uniform(low=-limit, high=limit, size=(x,y))

#activation functions

def sigmoid(x):
  z = x.copy()
  z[x < 0] = np.exp(x[x < 0])/(1 + np.exp(x[x<0]))
  z[x >= 0] = 1/(1+np.exp(-x[x >= 0]))
  return z

def d_sigmoid(x):
  return sigmoid(x)*(1 - sigmoid(x))

def softmax(x):
  e_x = np.exp(x - max(x))
  return e_x / e_x.sum()

def tanh(x):
  return np.tanh(x)
  
def d_tanh(x):
  return 1 - tanh(x)**2

def relu(x):
  return np.maximum(x,0)

def d_relu(x):
  z = x.copy()
  z[x < 0]=0
  z[x > 0]=1
  return z

actFn_dict = {"sigmoid":sigmoid,
               "softmax":softmax,
               "relu":relu,
               "tanh":tanh}
d_actFn_dict = {"sigmoid":d_sigmoid,
               "relu":d_relu,
               "tanh":d_tanh}

class OptimizerHelper:
  def forwardPropagation(self, W, B, X, num_layer, activation):
    H=list()
    A=list()
    a = np.array(W[0]@X.T + B[0])
    h = actFn_dict[activation](a)
    H.append(h)
    A.append(a)
    cur_ip = h

    for i in range(num_layer-1):
      a = np.array(W[i+1]@cur_ip + B[i+1])
      h = actFn_dict[activation](a.T).T
      H.append(h)
      A.append(a)
      cur_ip = h
    a = np.array(W[num_layer]@cur_ip + B[num_layer])
    h = a.T
    A.append(a)
    y = np.array([actFn_dict["softmax"](i) for i in h]).T
    return H, A, y

  def backwardPropagation(self, H, A, y, W, B, X, Y, num_layer, num_label, activation, wt_decay, loss_fn):
    xdim = X.shape[0]
    d_aL = y
    if(loss_fn == "crossEntropy"):
      ey = np.zeros([xdim,num_label], dtype= int)
      for i in range(xdim):
        ey[i][Y[i]] = 1
      d_aL = -(ey.T - y) 
    else:
      ey = np.zeros([xdim,num_label], dtype= int)
      for i in range(xdim):
        ey[i][Y[i]] = 1
      temp = -(ey.T - y)
      d_aL = 2*temp*y*(1-y)

    D_W = list()
    D_B = list()
    for k in reversed(range(1,num_layer+1)):
      d_wL = d_aL@H[k-1].T
      D_W.append(d_wL/xdim)

      d_bL = np.sum(d_aL, axis = 1).reshape(len(d_aL),1)/xdim
      D_B.append(d_bL)
  
      d_hL = W[k].T@d_aL
      dga = d_actFn_dict[activation](A[k-1])
      d_aL = np.multiply(d_hL,dga)

    d_wL = d_aL @ X
    d_bL = d_aL
    d_bL = np.sum(d_bL, axis = 1).reshape(len(d_aL),1)/xdim
    D_W.append(d_wL/xdim)
    D_B.append(d_bL)
    return D_W, D_B


class feedforwardNeuralNetwork:
  def __init__(self, X_train, Y_train, X_val, Y_val, num_layers = 4, hidden_size = 128, num_labels = 10, neta = 0.001, epochs = 15, batch = 64, w_init = "xavier", activation = 'tanh', wt_decay = 0, loss_fn = "crossEntropy"):
    self.num_layers = num_layers
    self.hidden_size = hidden_size
    self.xdim = X_train.shape[0]
    self.ydim = X_train.shape[1]
    self.num_labels = num_labels
    self.neta = neta
    self.epochs = epochs
    self.X_train = X_train
    self.Y_train = Y_train
    self.X_val = X_val
    self.Y_val = Y_val
    self.batch = batch
    w, b = self.initialiseWeight(w_init)
    self.W = w
    self.B = b
    self.activation = activation
    self.wt_decay = wt_decay
    self.loss_fn = loss_fn
  
  def initialiseWeight(self, method):
    w = list()
    b = list()
    if(method == "xavier"):
      w.append(xavierRandom(self.hidden_size, self.ydim))
      b.append(np.zeros([self.hidden_size,1]))
      for i in range(self.num_layers-1):
        w.append(xavierRandom(self.hidden_size, self.hidden_size))
        b.append(np.zeros([self.hidden_size,1]))
      w.append(xavierRandom(self.num_labels, self.hidden_size))
      b.append(np.zeros([self.num_labels,1]))
    else:
      # w.append(np.random.randn(hidden_size, ydim))
      w.append(np.random.uniform(-1,1, (self.hidden_size, self.ydim)))

      # b.append(np.random.randn(hidden_size,1))
      b.append(np.zeros([self.hidden_size,1]))
      for i in range(self.num_layers-1):
        # w.append(np.random.randn(hidden_size, hidden_size))
        w.append(np.random.uniform(-1,1, (self.hidden_size, self.hidden_size)))
        # b.append(np.random.randn(hidden_size,1))
        b.append(np.zeros([self.hidden_size,1]))
      # w.append(np.random.randn(num_labels, hidden_size))
      w.append(np.random.uniform(-1,1, (self.num_labels, self.hidden_size)))
      # b.append(np.random.randn(num_labels,1))
      b.append(np.zeros([self.num_labels,1])) 
    return w, b

  def calculateCrossEntropy(self, y_hat, Y):
    num_data = y_hat.shape[1]
    Y_one_hot = np.eye(self.num_labels)[Y]
    loss = np.sum(Y_one_hot * np.log(y_hat.T + 1e-15) )
    w_sq = 0
    for i in range(len(self.W)):
      w_sq += np.sum(self.W[i]**2)
    return (-loss + (self.wt_decay*w_sq)/2)/(num_data)

  def calculateMSE(self, y_hat, Y):
    m,n = y_hat.shape
    Y_one_hot = np.eye(self.num_labels)[Y]
    temp = (y_hat.T - Y_one_hot)**2
    loss = np.sum(temp)
    w_sq = 0
    for i in range(len(self.W)):
      w_sq += np.sum(self.W[i]**2)
    return (loss + (self.wt_decay*w_sq)/2)/(n)

  def calculateAccuracy(self, y_hat, Y):
    pred = np.argmax(y_hat, axis=0)
    accuracy = np.mean(pred==Y)
    return accuracy*100

  def feedForward(self, x):
    a = np.array(self.W[0]@x.T + self.B[0])
    h = actFn_dict[self.activation](a.T).T
    cur_ip = h

    for i in range(self.num_layers-1):
      a = np.array(self.W[i+1]@cur_ip + self.B[i+1])
      h = actFn_dict[self.activation](a.T).T
      cur_ip = h
    a = np.array(self.W[self.num_layers]@cur_ip + self.B[self.num_layers])
    h = a.T
    # print(h)
    y = np.array([actFn_dict["softmax"](i) for i in h]).T
    # print(y)
    return y

  # def gradientDescent(self):
  #   for k in range(self.epochs):
  #     O = OptimizerHelper()
  #     H, A, y = O.forwardPropagation(self.W, self.B, self.X_train, self.num_layers)
  #     D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, self.X_train, self.Y_train, self.num_layers, self.num_labels)
  #     D_W = np.flip(D_W)
  #     D_B = np.flip(D_B)
  #     # print(self.B[0].shape)
  #     # print(D_B[0].shape)
  #     for i in range(self.num_layers+1):
  #       self.W[i] = self.W[i] - self.neta*D_W[i]
  #       self.B[i] = self.B[i] - self.neta*D_B[i] 
      
  #     y_hat = self.feedForward(self.X_train)
  #     Y_one_hot = np.eye(self.num_labels)[self.Y_train]
  #     loss = -np.sum(Y_one_hot * np.log(y_hat.T + 1e-15) )
  #     print("Epoch {} : {}".format(k, (loss/self.xdim)))
  

  def momentumGD(self, beta = 0.9, eta = 1.0):
    # print("neta ",self.neta)
    O = OptimizerHelper()
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        if k==0:
          uw = D_W
          ub = D_B
        else:
          uw = beta*prev_uw + eta*D_W
          ub = beta*prev_ub + eta*D_B
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta*uw[i] - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta*ub[i] - self.neta*self.wt_decay*self.B[i]
        prev_uw = uw
        prev_ub = ub 
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
      
      
    return train_loss, train_acc, val_loss, val_acc

  def nagGD(self, beta = 0.1, eta = 1.0):
    O = OptimizerHelper()
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        if k==0:
          H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
          D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        else:
          u_w = beta*prev_uw
          u_b = beta*prev_ub
          temp_u_w = list()
          temp_u_b = list()
          for i in range(len(self.W)):
            temp_u_w.append(self.W[i]-u_w[i])
            temp_u_b.append(self.B[i]-u_b[i])  
          H, A, y = O.forwardPropagation(temp_u_w, temp_u_b, X_batch, self.num_layers, self.activation)
          D_W, D_B = O.backwardPropagation(H, A, y, temp_u_w, temp_u_b, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation)

        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        if k==0:
          uw = D_W
          ub = D_B
        else:
          uw = beta*prev_uw + eta*D_W
          ub = beta*prev_ub + eta*D_B
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta*uw[i] - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta*ub[i] - self.neta*self.wt_decay*self.B[i]
        prev_uw = uw
        prev_ub = ub 
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
    return train_loss, train_acc, val_loss, val_acc
    
  
  def stochasticGD(self):
    O = OptimizerHelper()
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        # print(self.B[0].shape)
        # print(D_B[0].shape)
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta*D_W[i] - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta*D_B[i] - self.neta*self.wt_decay*self.B[i]
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
    return train_loss, train_acc, val_loss, val_acc

  def RMSprop(self, beta = 0.9, eps = 1e-8):
    # print("neta ",self.neta)
    O = OptimizerHelper()
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        if k==0:
          uw = (1-beta)*np.square(D_W)
          ub = (1-beta)*np.square(D_B)
        else:
          uw = beta*prev_uw + (1-beta)*np.square(D_W)
          ub = beta*prev_ub + (1-beta)*np.square(D_B)
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta*D_W[i]/(np.sqrt(uw[i]) + eps) - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta*D_B[i]/(np.sqrt(ub[i]) + eps) - self.neta*self.wt_decay*self.B[i]
        prev_uw = uw
        prev_ub = ub 
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
    return train_loss, train_acc, val_loss, val_acc

  def adam(self, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):
    # print("neta ",self.neta)
    O = OptimizerHelper()
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        if k==0:
          mw = (1-beta1)*D_W
          mb = (1-beta1)*D_B
          uw = (1-beta2)*np.square(D_W)
          ub = (1-beta2)*np.square(D_B)
        else:
          mw = beta1*mw + (1-beta1)*D_W
          mb = beta1*mb + (1-beta1)*D_B
          uw = beta2*uw + (1-beta2)*np.square(D_W)
          ub = beta2*ub + (1-beta2)*np.square(D_B)

        mw_hat = mw / (1-np.power(beta1,k+1))
        mb_hat = mb / (1-np.power(beta1,k+1))
        uw_hat = uw / (1-np.power(beta2,k+1))
        ub_hat = ub / (1-np.power(beta2,k+1))
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta*mw_hat[i]/(np.sqrt(uw_hat[i]) + eps) - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta*mb_hat[i]/(np.sqrt(ub_hat[i]) + eps) - self.neta*self.wt_decay*self.B[i]
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
    return train_loss, train_acc, val_loss, val_acc

  def nAdam(self, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):
    # print("neta ",self.neta)
    val_loss = list()
    val_acc = list()
    train_loss = list()
    train_acc = list()
    O = OptimizerHelper()
    for k in range(self.epochs):
      for b in range(0, self.xdim, self.batch):
        X_batch = self.X_train[b:b+self.batch]
        Y_batch = self.Y_train[b:b+self.batch]
        H, A, y = O.forwardPropagation(self.W, self.B, X_batch, self.num_layers, self.activation)
        D_W, D_B = O.backwardPropagation(H, A, y, self.W, self.B, X_batch, Y_batch, self.num_layers, self.num_labels, self.activation, self.wt_decay, self.loss_fn)
        D_W = np.flip(D_W)
        D_B = np.flip(D_B)
        if k==0:
          mw = (1-beta1)*D_W
          mb = (1-beta1)*D_B
          uw = (1-beta2)*np.square(D_W)
          ub = (1-beta2)*np.square(D_B)
        else:
          mw = beta1*mw + (1-beta1)*D_W
          mb = beta1*mb + (1-beta1)*D_B
          uw = beta2*uw + (1-beta2)*np.square(D_W)
          ub = beta2*ub + (1-beta2)*np.square(D_B)

        mw_hat = mw / (1-np.power(beta1,k+1))
        mb_hat = mb / (1-np.power(beta1,k+1))
        uw_hat = uw / (1-np.power(beta2,k+1))
        ub_hat = ub / (1-np.power(beta2,k+1))
        for i in range(self.num_layers+1):
          self.W[i] = self.W[i] - self.neta/(np.sqrt(uw_hat[i]) + eps)*(beta1*mw_hat[i] + (1-beta1)*D_W[i] / (1-np.power(beta1,k+1))) - self.neta*self.wt_decay*self.W[i]
          self.B[i] = self.B[i] - self.neta/(np.sqrt(ub_hat[i]) + eps)*(beta1*mb_hat[i] + (1-beta1)*D_B[i] / (1-np.power(beta1,k+1))) - self.neta*self.wt_decay*self.B[i] 
      
      y_train_hat = self.feedForward(self.X_train)
      y_val_hat = self.feedForward(self.X_val)
      train_acc.append(self.calculateAccuracy(y_train_hat, self.Y_train))
      val_acc.append(self.calculateAccuracy(y_val_hat, self.Y_val))
      if(self.loss_fn == "crossEntropy"):
        train_loss.append(self.calculateCrossEntropy(y_train_hat, self.Y_train))
        val_loss.append(self.calculateCrossEntropy(y_val_hat, self.Y_val))
      else:
        # print("---------MSE-------------")
        train_loss.append(self.calculateMSE(y_train_hat, self.Y_train))
        val_loss.append(self.calculateMSE(y_val_hat, self.Y_val))
      print("Epoch {} : Loss:{}\t Accuracy:{}".format(k, train_loss[-1], train_acc[-1]))
    return train_loss, train_acc, val_loss, val_acc